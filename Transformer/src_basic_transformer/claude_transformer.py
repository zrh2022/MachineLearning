import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Optional, Tuple


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°"""

    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % n_heads == 0  # ç¡®ä¿d_modelèƒ½è¢«n_headsæ•´é™¤

        self.d_model = d_model  # æ¨¡å‹ç»´åº¦
        self.n_heads = n_heads  # æ³¨æ„åŠ›å¤´æ•°
        self.d_k = d_model // n_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # çº¿æ€§å˜æ¢å±‚ï¼šQ, K, V
        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Queryæƒé‡çŸ©é˜µ
        self.w_k = nn.Linear(d_model, d_model, bias=False)  # Keyæƒé‡çŸ©é˜µ
        self.w_v = nn.Linear(d_model, d_model, bias=False)  # Valueæƒé‡çŸ©é˜µ
        self.w_o = nn.Linear(d_model, d_model, bias=False)  # è¾“å‡ºæƒé‡çŸ©é˜µ

        self.dropout = nn.Dropout(dropout)  # Dropoutå±‚
        self.scale = math.sqrt(self.d_k)  # ç¼©æ”¾å› å­ï¼Œé˜²æ­¢softmaxé¥±å’Œ

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            query: [batch_size, seq_len, d_model]
            key: [batch_size, seq_len, d_model]
            value: [batch_size, seq_len, d_model]
            mask: [batch_size, seq_len, seq_len] æˆ– None
        Returns:
            output: [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, d_model = query.shape

        # åˆ†åˆ«è·å–åºåˆ—é•¿åº¦
        tgt_seq_len = query.size(1)  # ç›®æ ‡åºåˆ—é•¿åº¦
        src_seq_len = key.size(1)  # æºåºåˆ—é•¿åº¦

        # çº¿æ€§å˜æ¢å¾—åˆ°Q, K, V: [batch_size, seq_len, d_model]
        Q = self.w_q(query)
        K = self.w_k(key)
        V = self.w_v(value)

        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼: [batch_size, n_heads, seq_len, d_k]
        Q = Q.view(batch_size, tgt_seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, src_seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, src_seq_len, self.n_heads, self.d_k).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›
        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)  # [batch_size, n_heads, seq_len, d_k]

        # æ‹¼æ¥å¤šå¤´ç»“æœ: [batch_size, seq_len, d_model]
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )

        # æœ€ç»ˆçº¿æ€§å˜æ¢: [batch_size, seq_len, d_model]
        output = self.w_o(attention_output)

        return output

    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor,
                                     V: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        Args:
            Q: [batch_size, n_heads, seq_len, d_k]
            K: [batch_size, n_heads, seq_len, d_k]
            V: [batch_size, n_heads, seq_len, d_k]
            mask: [batch_size, seq_len, seq_len] æˆ– None
        Returns:
            output: [batch_size, n_heads, seq_len, d_k]
        """
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: [batch_size, n_heads, seq_len, seq_len]
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # åº”ç”¨æ©ç ï¼ˆå¦‚æœæä¾›ï¼‰
        if mask is not None:
            # æ‰©å±•maskç»´åº¦ä»¥åŒ¹é…scoresçš„å½¢çŠ¶: [batch_size, 1, seq_len, seq_len]
            mask = mask.unsqueeze(1)
            scores.masked_fill_(mask == 0, -1e9)  # å°†æ©ç ä½ç½®è®¾ä¸ºå¾ˆå¤§çš„è´Ÿå€¼

        # åº”ç”¨softmax: [batch_size, n_heads, seq_len, seq_len]
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)  # åº”ç”¨dropout

        # è®¡ç®—åŠ æƒå€¼: [batch_size, n_heads, seq_len, d_k]
        output = torch.matmul(attention_weights, V)

        return output


class PositionwiseFeedForward(nn.Module):
    """ä½ç½®å‰é¦ˆç¥ç»ç½‘ç»œ"""

    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)  # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
        self.linear2 = nn.Linear(d_ff, d_model)  # ç¬¬äºŒä¸ªçº¿æ€§å±‚
        self.dropout = nn.Dropout(dropout)  # Dropoutå±‚

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, seq_len, d_model]
        Returns:
            output: [batch_size, seq_len, d_model]
        """
        # FFN(x) = max(0, xW1 + b1)W2 + b2
        # ç¬¬ä¸€å±‚ + ReLU + Dropout: [batch_size, seq_len, d_ff]
        hidden = self.dropout(F.relu(self.linear1(x)))
        # ç¬¬äºŒå±‚: [batch_size, seq_len, d_model]
        output = self.linear2(hidden)
        return output


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """

    def __init__(self, d_model: int, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ: [max_len, d_model]
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()  # [max_len, 1]

        # è®¡ç®—é™¤æ•°é¡¹
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             -(math.log(10000.0) / d_model))  # [d_model//2]

        # åº”ç”¨sinåˆ°å¶æ•°ç´¢å¼•
        pe[:, 0::2] = torch.sin(position * div_term)  # [max_len, d_model//2]
        # åº”ç”¨cosåˆ°å¥‡æ•°ç´¢å¼•
        pe[:, 1::2] = torch.cos(position * div_term)  # [max_len, d_model//2]

        # æ·»åŠ batchç»´åº¦: [1, max_len, d_model]
        pe = pe.unsqueeze(0)

        # æ³¨å†Œä¸ºbufferï¼Œä¸å‚ä¸æ¢¯åº¦è®¡ç®—ä½†ä¼šä¿å­˜åˆ°æ¨¡å‹çŠ¶æ€
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, seq_len, d_model]
        Returns:
            output: [batch_size, seq_len, d_model]
        """
        seq_len = x.size(1)
        # æ·»åŠ ä½ç½®ç¼–ç : [batch_size, seq_len, d_model]
        x = x + self.pe[:, :seq_len].detach()  # detaché˜²æ­¢ä½ç½®ç¼–ç å‚ä¸æ¢¯åº¦è®¡ç®—
        return x


class LayerNorm(nn.Module):
    """å±‚å½’ä¸€åŒ–"""

    def __init__(self, d_model: int, eps: float = 1e-6):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))  # ç¼©æ”¾å‚æ•°
        self.beta = nn.Parameter(torch.zeros(d_model))  # åç§»å‚æ•°
        self.eps = eps  # é˜²æ­¢é™¤é›¶çš„å°æ•°å€¼

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, seq_len, d_model]
        Returns:
            output: [batch_size, seq_len, d_model]
        """
        # è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼ˆåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼‰
        mean = x.mean(-1, keepdim=True)  # [batch_size, seq_len, 1]
        std = x.std(-1, keepdim=True)  # [batch_size, seq_len, 1]

        # å½’ä¸€åŒ–: [batch_size, seq_len, d_model]
        normalized = (x - mean) / (std + self.eps)

        # åº”ç”¨å¯å­¦ä¹ å‚æ•°: [batch_size, seq_len, d_model]
        output = self.gamma * normalized + self.beta

        return output


class EncoderLayer(nn.Module):
    """Transformerç¼–ç å™¨å±‚"""

    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super(EncoderLayer, self).__init__()

        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)  # è‡ªæ³¨æ„åŠ›
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)  # å‰é¦ˆç½‘ç»œ
        self.norm1 = LayerNorm(d_model)  # ç¬¬ä¸€ä¸ªå±‚å½’ä¸€åŒ–
        self.norm2 = LayerNorm(d_model)  # ç¬¬äºŒä¸ªå±‚å½’ä¸€åŒ–
        self.dropout = nn.Dropout(dropout)  # Dropoutå±‚

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, seq_len, d_model]
            mask: [batch_size, seq_len, seq_len] æˆ– None
        Returns:
            output: [batch_size, seq_len, d_model]
        """
        # ç¬¬ä¸€ä¸ªå­å±‚ï¼šè‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_output = self.self_attention(x, x, x, mask)  # [batch_size, seq_len, d_model]
        x = self.norm1(x + self.dropout(attn_output))  # [batch_size, seq_len, d_model]

        # ç¬¬äºŒä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(x)  # [batch_size, seq_len, d_model]
        x = self.norm2(x + self.dropout(ff_output))  # [batch_size, seq_len, d_model]

        return x


class DecoderLayer(nn.Module):
    """Transformerè§£ç å™¨å±‚"""

    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super(DecoderLayer, self).__init__()

        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)  # æ©ç è‡ªæ³¨æ„åŠ›
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)  # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)  # å‰é¦ˆç½‘ç»œ
        self.norm1 = LayerNorm(d_model)  # ç¬¬ä¸€ä¸ªå±‚å½’ä¸€åŒ–
        self.norm2 = LayerNorm(d_model)  # ç¬¬äºŒä¸ªå±‚å½’ä¸€åŒ–
        self.norm3 = LayerNorm(d_model)  # ç¬¬ä¸‰ä¸ªå±‚å½’ä¸€åŒ–
        self.dropout = nn.Dropout(dropout)  # Dropoutå±‚

    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,
                self_attn_mask: Optional[torch.Tensor] = None,
                cross_attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            x: [batch_size, tgt_seq_len, d_model] - ç›®æ ‡åºåˆ—
            encoder_output: [batch_size, src_seq_len, d_model] - ç¼–ç å™¨è¾“å‡º
            self_attn_mask: [batch_size, tgt_seq_len, tgt_seq_len] - è‡ªæ³¨æ„åŠ›æ©ç 
            cross_attn_mask: [batch_size, tgt_seq_len, src_seq_len] - äº¤å‰æ³¨æ„åŠ›æ©ç 
        Returns:
            output: [batch_size, tgt_seq_len, d_model]
        """
        # ç¬¬ä¸€ä¸ªå­å±‚ï¼šæ©ç è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        self_attn_output = self.self_attention(x, x, x, self_attn_mask)  # [batch_size, tgt_seq_len, d_model]
        x = self.norm1(x + self.dropout(self_attn_output))  # [batch_size, tgt_seq_len, d_model]

        # ç¬¬äºŒä¸ªå­å±‚ï¼šç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        cross_attn_output = self.cross_attention(x, encoder_output, encoder_output,
                                                 cross_attn_mask)  # [batch_size, tgt_seq_len, d_model]
        x = self.norm2(x + self.dropout(cross_attn_output))  # [batch_size, tgt_seq_len, d_model]

        # ç¬¬ä¸‰ä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(x)  # [batch_size, tgt_seq_len, d_model]
        x = self.norm3(x + self.dropout(ff_output))  # [batch_size, tgt_seq_len, d_model]

        return x


class TransformerEncoder(nn.Module):
    """Transformerç¼–ç å™¨"""

    def __init__(self, vocab_size: int, d_model: int, n_heads: int,
                 n_layers: int, d_ff: int, max_len: int = 5000, dropout: float = 0.1):
        super(TransformerEncoder, self).__init__()

        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)  # è¯åµŒå…¥å±‚
        self.positional_encoding = PositionalEncoding(d_model, max_len)  # ä½ç½®ç¼–ç 

        # å †å å¤šä¸ªç¼–ç å™¨å±‚
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        self.dropout = nn.Dropout(dropout)  # Dropoutå±‚

    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            src: [batch_size, src_seq_len] - æºåºåˆ—token ids
            src_mask: [batch_size, src_seq_len, src_seq_len] - æºåºåˆ—æ©ç 
        Returns:
            output: [batch_size, src_seq_len, d_model]
        """
        # è¯åµŒå…¥ + ç¼©æ”¾ + ä½ç½®ç¼–ç : [batch_size, src_seq_len, d_model]
        x = self.embedding(src) * math.sqrt(self.d_model)  # è®ºæ–‡ä¸­çš„ç¼©æ”¾æŠ€å·§
        x = self.positional_encoding(x)  # [batch_size, src_seq_len, d_model]
        x = self.dropout(x)  # [batch_size, src_seq_len, d_model]

        # é€šè¿‡æ‰€æœ‰ç¼–ç å™¨å±‚
        for layer in self.layers:
            x = layer(x, src_mask)  # [batch_size, src_seq_len, d_model]

        return x


class TransformerDecoder(nn.Module):
    """Transformerè§£ç å™¨"""

    def __init__(self, vocab_size: int, d_model: int, n_heads: int,
                 n_layers: int, d_ff: int, max_len: int = 5000, dropout: float = 0.1):
        super(TransformerDecoder, self).__init__()

        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)  # è¯åµŒå…¥å±‚
        self.positional_encoding = PositionalEncoding(d_model, max_len)  # ä½ç½®ç¼–ç 

        # å †å å¤šä¸ªè§£ç å™¨å±‚
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        self.dropout = nn.Dropout(dropout)  # Dropoutå±‚

    def forward(self, tgt: torch.Tensor, encoder_output: torch.Tensor,
                tgt_mask: Optional[torch.Tensor] = None,
                src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            tgt: [batch_size, tgt_seq_len] - ç›®æ ‡åºåˆ—token ids
            encoder_output: [batch_size, src_seq_len, d_model] - ç¼–ç å™¨è¾“å‡º
            tgt_mask: [batch_size, tgt_seq_len, tgt_seq_len] - ç›®æ ‡åºåˆ—æ©ç 
            src_mask: [batch_size, tgt_seq_len, src_seq_len] - æºåºåˆ—æ©ç 
        Returns:
            output: [batch_size, tgt_seq_len, d_model]
        """
        # è¯åµŒå…¥ + ç¼©æ”¾ + ä½ç½®ç¼–ç : [batch_size, tgt_seq_len, d_model]
        x = self.embedding(tgt) * math.sqrt(self.d_model)  # è®ºæ–‡ä¸­çš„ç¼©æ”¾æŠ€å·§
        x = self.positional_encoding(x)  # [batch_size, tgt_seq_len, d_model]
        x = self.dropout(x)  # [batch_size, tgt_seq_len, d_model]

        # é€šè¿‡æ‰€æœ‰è§£ç å™¨å±‚
        for layer in self.layers:
            x = layer(x, encoder_output, tgt_mask, src_mask)  # [batch_size, tgt_seq_len, d_model]

        return x


class Transformer(nn.Module):
    """å®Œæ•´çš„Transformeræ¨¡å‹"""

    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 512,
                 n_heads: int = 8, n_layers: int = 6, d_ff: int = 2048,
                 max_len: int = 5000, dropout: float = 0.1):
        super(Transformer, self).__init__()

        # ç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder = TransformerEncoder(src_vocab_size, d_model, n_heads,
                                          n_layers, d_ff, max_len, dropout)
        self.decoder = TransformerDecoder(tgt_vocab_size, d_model, n_heads,
                                          n_layers, d_ff, max_len, dropout)

        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(d_model, tgt_vocab_size, bias=False)  # [d_model] -> [tgt_vocab_size]

        # æƒé‡å…±äº«ï¼šåµŒå…¥å±‚å’Œè¾“å‡ºæŠ•å½±å±‚å…±äº«æƒé‡ï¼ˆè®ºæ–‡ä¸­çš„ä¼˜åŒ–æŠ€å·§ï¼‰
        self.output_projection.weight = self.decoder.embedding.weight

        # å‚æ•°åˆå§‹åŒ–
        self.init_parameters()

    def init_parameters(self):
        """å‚æ•°åˆå§‹åŒ– - ä½¿ç”¨Xavierå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src: torch.Tensor, tgt: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None,
                src_key_padding_mask: Optional[torch.Tensor] = None,
                tgt_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        Args:
            src: [batch_size, src_seq_len] - æºåºåˆ—
            tgt: [batch_size, tgt_seq_len] - ç›®æ ‡åºåˆ—
            src_mask: [batch_size, src_seq_len, src_seq_len] - æºåºåˆ—æ³¨æ„åŠ›æ©ç 
            tgt_mask: [batch_size, tgt_seq_len, tgt_seq_len] - ç›®æ ‡åºåˆ—æ³¨æ„åŠ›æ©ç 
            src_key_padding_mask: [batch_size, src_seq_len] - æºåºåˆ—å¡«å……æ©ç 
            tgt_key_padding_mask: [batch_size, tgt_seq_len] - ç›®æ ‡åºåˆ—å¡«å……æ©ç 
        Returns:
            output: [batch_size, tgt_seq_len, tgt_vocab_size] - è¾“å‡ºlogits
        """
        # ç¼–ç å™¨å‰å‘ä¼ æ’­: [batch_size, src_seq_len, d_model]
        encoder_output = self.encoder(src, src_mask)

        # è§£ç å™¨å‰å‘ä¼ æ’­: [batch_size, tgt_seq_len, d_model]
        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)

        # è¾“å‡ºæŠ•å½±: [batch_size, tgt_seq_len, tgt_vocab_size]
        output = self.output_projection(decoder_output)

        return output

    def generate_square_subsequent_mask(self, sz: int, device: torch.device) -> torch.Tensor:
        """
        ç”Ÿæˆä¸‹ä¸‰è§’æ©ç çŸ©é˜µï¼Œç”¨äºé˜²æ­¢è§£ç å™¨çœ‹åˆ°æœªæ¥ä¿¡æ¯
        Args:
            sz: åºåˆ—é•¿åº¦
            device: è®¾å¤‡
        Returns:
            mask: [sz, sz] - ä¸‹ä¸‰è§’æ©ç çŸ©é˜µ
        """
        mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1)  # ä¸Šä¸‰è§’çŸ©é˜µ
        return mask == 0  # è½¬æ¢ä¸ºä¸‹ä¸‰è§’æ©ç ï¼ˆTrueè¡¨ç¤ºå…è®¸æ³¨æ„åŠ›ï¼‰

    def create_padding_mask(self, seq: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:
        """
        åˆ›å»ºå¡«å……æ©ç 
        Args:
            seq: [batch_size, seq_len] - è¾“å…¥åºåˆ—
            pad_idx: å¡«å……tokençš„ç´¢å¼•
        Returns:
            mask: [batch_size, seq_len] - å¡«å……æ©ç ï¼ˆTrueè¡¨ç¤ºæœ‰æ•ˆtokenï¼‰
        """
        return seq != pad_idx


def test_transformer():
    """æµ‹è¯•Transformeræ¨¡å‹"""
    print("å¼€å§‹æµ‹è¯•Transformeræ¨¡å‹...")

    # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ¨¡å‹å‚æ•°
    src_vocab_size = 1000  # æºè¯­è¨€è¯æ±‡è¡¨å¤§å°
    tgt_vocab_size = 1000  # ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨å¤§å°
    d_model = 512  # æ¨¡å‹ç»´åº¦
    n_heads = 8  # æ³¨æ„åŠ›å¤´æ•°
    n_layers = 6  # å±‚æ•°
    d_ff = 2048  # å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦
    max_len = 100  # æœ€å¤§åºåˆ—é•¿åº¦
    dropout = 0.1  # Dropoutæ¦‚ç‡

    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPU
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=d_model,
        n_heads=n_heads,
        n_layers=n_layers,
        d_ff=d_ff,
        max_len=max_len,
        dropout=dropout
    ).to(device)

    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 32
    src_seq_len = 20
    tgt_seq_len = 25

    # éšæœºç”Ÿæˆæºåºåˆ—å’Œç›®æ ‡åºåˆ— (é¿å…ä½¿ç”¨pad_idx=0)
    src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len)).to(device)  # [batch_size, src_seq_len]
    tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len)).to(device)  # [batch_size, tgt_seq_len]

    print(f"æºåºåˆ—å½¢çŠ¶: {src.shape}")
    print(f"ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}")

    # åˆ›å»ºæ©ç 
    # ç›®æ ‡åºåˆ—çš„å› æœæ©ç ï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
    tgt_mask = model.generate_square_subsequent_mask(tgt_seq_len, device)  # [tgt_seq_len, tgt_seq_len]
    tgt_mask = tgt_mask.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, tgt_seq_len, tgt_seq_len]

    print(f"ç›®æ ‡æ©ç å½¢çŠ¶: {tgt_mask.shape}")

    # å‰å‘ä¼ æ’­æµ‹è¯•
    print("\nè¿›è¡Œå‰å‘ä¼ æ’­...")
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    with torch.no_grad():
        output = model(src, tgt, tgt_mask=tgt_mask)  # [batch_size, tgt_seq_len, tgt_vocab_size]

    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")

    # è®¡ç®—è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
    output_probs = F.softmax(output, dim=-1)  # [batch_size, tgt_seq_len, tgt_vocab_size]
    print(f"æ¦‚ç‡åˆ†å¸ƒå’Œ (åº”è¯¥æ¥è¿‘1.0): {output_probs.sum(dim=-1)[0, 0].item():.6f}")

    # æµ‹è¯•è®­ç»ƒæ¨¡å¼
    print("\næµ‹è¯•è®­ç»ƒæ¨¡å¼...")
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

    # åˆ›å»ºç›®æ ‡æ ‡ç­¾ï¼ˆç”¨äºè®¡ç®—æŸå¤±ï¼‰
    tgt_labels = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len)).to(device)

    # å‰å‘ä¼ æ’­
    output = model(src, tgt, tgt_mask=tgt_mask)  # [batch_size, tgt_seq_len, tgt_vocab_size]

    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    # éœ€è¦é‡å¡‘å¼ é‡ä»¥åŒ¹é…æŸå¤±å‡½æ•°è¦æ±‚
    output_flat = output.view(-1, tgt_vocab_size)  # [batch_size * tgt_seq_len, tgt_vocab_size]
    tgt_labels_flat = tgt_labels.view(-1)  # [batch_size * tgt_seq_len]

    loss = F.cross_entropy(output_flat, tgt_labels_flat)
    print(f"äº¤å‰ç†µæŸå¤±: {loss.item():.4f}")

    # æµ‹è¯•åå‘ä¼ æ’­
    print("æµ‹è¯•åå‘ä¼ æ’­...")
    loss.backward()

    # æ£€æŸ¥æ¢¯åº¦
    has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°æœ‰æ¢¯åº¦: {has_grad}")

    # å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆä»…åœ¨CUDAå¯ç”¨æ—¶æ˜¾ç¤ºï¼‰
    if device.type == 'cuda':
        print(f"\nGPUå†…å­˜ä½¿ç”¨:")
        print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(device) / 1024 ** 2:.1f} MB")
        print(f"  å·²ç¼“å­˜: {torch.cuda.memory_reserved(device) / 1024 ** 2:.1f} MB")

    print("\nâœ… Transformeræ¨¡å‹æµ‹è¯•å®Œæˆï¼")


def test_individual_components():
    """æµ‹è¯•å„ä¸ªç»„ä»¶çš„åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å„ä¸ªç»„ä»¶...")
    print("=" * 50)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
    print("\n1. æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶...")
    d_model, n_heads = 512, 8
    seq_len, batch_size = 10, 4

    mha = MultiHeadAttention(d_model, n_heads).to(device)
    x = torch.randn(batch_size, seq_len, d_model).to(device)  # [batch_size, seq_len, d_model]

    # è‡ªæ³¨æ„åŠ›æµ‹è¯•
    attn_output = mha(x, x, x)  # [batch_size, seq_len, d_model]
    print(f"  è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"  è‡ªæ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {attn_output.shape}")
    print(f"  è¾“å‡ºèŒƒå›´: [{attn_output.min().item():.4f}, {attn_output.max().item():.4f}]")

    # æµ‹è¯•å¸¦æ©ç çš„æ³¨æ„åŠ›
    mask = torch.ones(batch_size, seq_len, seq_len).to(device)  # [batch_size, seq_len, seq_len]
    mask[:, :, 5:] = 0  # æ©ç›–ååŠéƒ¨åˆ†

    masked_output = mha(x, x, x, mask)  # [batch_size, seq_len, d_model]
    print(f"  å¸¦æ©ç çš„æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {masked_output.shape}")

    # æµ‹è¯•ä½ç½®ç¼–ç 
    print("\n2. æµ‹è¯•ä½ç½®ç¼–ç ...")
    pos_encoding = PositionalEncoding(d_model, max_len=100).to(device)
    x_with_pos = pos_encoding(x)  # [batch_size, seq_len, d_model]
    print(f"  ä½ç½®ç¼–ç åå½¢çŠ¶: {x_with_pos.shape}")
    print(f"  ä½ç½®ç¼–ç å½±å“: {(x_with_pos - x).abs().mean().item():.6f}")

    # æµ‹è¯•å‰é¦ˆç½‘ç»œ
    print("\n3. æµ‹è¯•ä½ç½®å‰é¦ˆç½‘ç»œ...")
    d_ff = 2048
    ffn = PositionwiseFeedForward(d_model, d_ff).to(device)
    ffn_output = ffn(x)  # [batch_size, seq_len, d_model]
    print(f"  å‰é¦ˆç½‘ç»œè¾“å‡ºå½¢çŠ¶: {ffn_output.shape}")
    print(f"  è¾“å‡ºèŒƒå›´: [{ffn_output.min().item():.4f}, {ffn_output.max().item():.4f}]")

    # æµ‹è¯•å±‚å½’ä¸€åŒ–
    print("\n4. æµ‹è¯•å±‚å½’ä¸€åŒ–...")
    layer_norm = LayerNorm(d_model).to(device)
    norm_output = layer_norm(x)  # [batch_size, seq_len, d_model]
    print(f"  å±‚å½’ä¸€åŒ–è¾“å‡ºå½¢çŠ¶: {norm_output.shape}")
    print(f"  å½’ä¸€åŒ–åå‡å€¼: {norm_output.mean(dim=-1)[0, 0].item():.6f}")
    print(f"  å½’ä¸€åŒ–åæ ‡å‡†å·®: {norm_output.std(dim=-1)[0, 0].item():.6f}")

    # æµ‹è¯•ç¼–ç å™¨å±‚
    print("\n5. æµ‹è¯•ç¼–ç å™¨å±‚...")
    encoder_layer = EncoderLayer(d_model, n_heads, d_ff).to(device)
    enc_output = encoder_layer(x)  # [batch_size, seq_len, d_model]
    print(f"  ç¼–ç å™¨å±‚è¾“å‡ºå½¢çŠ¶: {enc_output.shape}")

    # æµ‹è¯•è§£ç å™¨å±‚
    print("\n6. æµ‹è¯•è§£ç å™¨å±‚...")
    decoder_layer = DecoderLayer(d_model, n_heads, d_ff).to(device)
    # åˆ›å»ºç¼–ç å™¨è¾“å‡ºä½œä¸ºäº¤å‰æ³¨æ„åŠ›çš„è¾“å…¥
    encoder_out = torch.randn(batch_size, seq_len, d_model).to(device)  # [batch_size, seq_len, d_model]
    dec_output = decoder_layer(x, encoder_out)  # [batch_size, seq_len, d_model]
    print(f"  è§£ç å™¨å±‚è¾“å‡ºå½¢çŠ¶: {dec_output.shape}")

    print("\nâœ… æ‰€æœ‰ç»„ä»¶æµ‹è¯•å®Œæˆï¼")


def test_demonstrate_attention_patterns():
    """å±•ç¤ºæ³¨æ„åŠ›æ¨¡å¼"""
    print("\n" + "=" * 50)
    print("å±•ç¤ºæ³¨æ„åŠ›æ¨¡å¼...")
    print("=" * 50)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ›å»ºç®€å•çš„æ³¨æ„åŠ›å±‚
    d_model, n_heads = 64, 4
    seq_len, batch_size = 8, 1

    mha = MultiHeadAttention(d_model, n_heads, dropout=0.0).to(device)
    mha.eval()  # å…³é—­dropoutä»¥è·å¾—ç¡®å®šæ€§ç»“æœ

    # åˆ›å»ºæœ‰æ¨¡å¼çš„è¾“å…¥åºåˆ—
    x = torch.randn(batch_size, seq_len, d_model).to(device)

    # ä¿®æ”¹MultiHeadAttentionç±»ä»¥è¿”å›æ³¨æ„åŠ›æƒé‡
    class AttentionVisualization(MultiHeadAttention):
        def forward(self, query, key, value, mask=None):
            batch_size, seq_len, d_model = query.shape

            Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
            K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
            V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)

            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
            if mask is not None:
                mask = mask.unsqueeze(1)
                scores.masked_fill_(mask == 0, -1e9)

            # è·å–æ³¨æ„åŠ›æƒé‡
            attention_weights = F.softmax(scores, dim=-1)

            # è®¡ç®—è¾“å‡º
            attention_output = torch.matmul(attention_weights, V)
            attention_output = attention_output.transpose(1, 2).contiguous().view(
                batch_size, seq_len, self.d_model
            )
            output = self.w_o(attention_output)

            return output, attention_weights  # è¿”å›è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡

    # åˆ›å»ºå¯è§†åŒ–ç‰ˆæœ¬çš„æ³¨æ„åŠ›å±‚
    vis_mha = AttentionVisualization(d_model, n_heads, dropout=0.0).to(device)
    vis_mha.load_state_dict(mha.state_dict())  # å¤åˆ¶æƒé‡
    vis_mha.eval()

    with torch.no_grad():
        output, attention_weights = vis_mha(x, x, x)

    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")  # [batch_size, n_heads, seq_len, seq_len]

    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›æ¨¡å¼
    first_head_attention = attention_weights[0, 0].cpu().numpy()  # [seq_len, seq_len]
    print("\nç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å¤´çš„æ³¨æ„åŠ›çŸ©é˜µ:")
    print("(è¡Œ=æŸ¥è¯¢ä½ç½®, åˆ—=é”®ä½ç½®)")
    for i in range(seq_len):
        row_str = " ".join([f"{first_head_attention[i, j]:.3f}" for j in range(seq_len)])
        print(f"ä½ç½®{i}: {row_str}")

    # éªŒè¯æ³¨æ„åŠ›æƒé‡å½’ä¸€åŒ–
    attention_sums = attention_weights.sum(dim=-1)  # åº”è¯¥å…¨ä¸º1
    print(f"\næ³¨æ„åŠ›æƒé‡å’Œ (åº”è¯¥å…¨ä¸º1): {attention_sums.mean().item():.6f} Â± {attention_sums.std().item():.6f}")


def test_performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 50)
    print("æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    print("=" * 50)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ä¸åŒè§„æ¨¡çš„æ¨¡å‹é…ç½®
    configs = [
        {"name": "Small", "d_model": 256, "n_heads": 4, "n_layers": 3, "d_ff": 1024},
        {"name": "Base", "d_model": 512, "n_heads": 8, "n_layers": 6, "d_ff": 2048},
        {"name": "Large", "d_model": 768, "n_heads": 12, "n_layers": 12, "d_ff": 3072},
    ]

    batch_size = 16
    seq_len = 128
    vocab_size = 10000

    for config in configs:
        print(f"\næµ‹è¯• {config['name']} æ¨¡å‹:")
        print(f"  å‚æ•°: d_model={config['d_model']}, n_heads={config['n_heads']}, "
              f"n_layers={config['n_layers']}, d_ff={config['d_ff']}")

        # åˆ›å»ºæ¨¡å‹
        model = Transformer(
            src_vocab_size=vocab_size,
            tgt_vocab_size=vocab_size,
            d_model=config['d_model'],
            n_heads=config['n_heads'],
            n_layers=config['n_layers'],
            d_ff=config['d_ff'],
            dropout=0.1
        ).to(device)

        # è®¡ç®—å‚æ•°æ•°é‡
        param_count = sum(p.numel() for p in model.parameters())
        print(f"  å‚æ•°æ•°é‡: {param_count:,}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        src = torch.randint(1, vocab_size, (batch_size, seq_len)).to(device)
        tgt = torch.randint(1, vocab_size, (batch_size, seq_len)).to(device)
        tgt_mask = model.generate_square_subsequent_mask(seq_len, device)
        tgt_mask = tgt_mask.unsqueeze(0).expand(batch_size, -1, -1)

        # é¢„çƒ­
        model.train()
        for _ in range(3):
            output = model(src, tgt, tgt_mask=tgt_mask)

        if device.type == 'cuda':
            torch.cuda.synchronize()

        # æµ‹é‡å‰å‘ä¼ æ’­æ—¶é—´
        import time
        start_time = time.time()
        num_iterations = 10

        for _ in range(num_iterations):
            output = model(src, tgt, tgt_mask=tgt_mask)
            if device.type == 'cuda':
                torch.cuda.synchronize()

        forward_time = (time.time() - start_time) / num_iterations
        print(f"  å¹³å‡å‰å‘ä¼ æ’­æ—¶é—´: {forward_time * 1000:.2f} ms")

        # æµ‹é‡å†…å­˜ä½¿ç”¨
        if device.type == 'cuda':
            memory_mb = torch.cuda.max_memory_allocated(device) / 1024 ** 2
            print(f"  å³°å€¼GPUå†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
            torch.cuda.reset_peak_memory_stats()

        # æ¸…ç†
        del model
        if device.type == 'cuda':
            torch.cuda.empty_cache()


def test_training_example():
    """ç®€å•çš„è®­ç»ƒç¤ºä¾‹"""
    print("\n" + "=" * 50)
    print("ç®€å•è®­ç»ƒç¤ºä¾‹...")
    print("=" * 50)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åˆ›å»ºå°å‹æ¨¡å‹ç”¨äºæ¼”ç¤º
    model = Transformer(
        src_vocab_size=1000,
        tgt_vocab_size=1000,
        d_model=256,
        n_heads=4,
        n_layers=2,
        d_ff=1024,
        dropout=0.1
    ).to(device)

    # ä¼˜åŒ–å™¨ - ä½¿ç”¨Adamä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - è®ºæ–‡ä¸­çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
    class TransformerLRScheduler:
        def __init__(self, optimizer, d_model, warmup_steps=4000):
            self.optimizer = optimizer
            self.d_model = d_model
            self.warmup_steps = warmup_steps
            self.step_num = 0

        def step(self):
            self.step_num += 1
            lr = self.d_model ** (-0.5) * min(self.step_num ** (-0.5),
                                              self.step_num * self.warmup_steps ** (-1.5))
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
            return lr

    scheduler = TransformerLRScheduler(optimizer, model.encoder.d_model)

    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    batch_size = 16
    src_seq_len = 32
    tgt_seq_len = 32
    num_batches = 5

    model.train()

    for batch_idx in range(num_batches):
        # ç”Ÿæˆéšæœºæ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­è¿™é‡Œæ˜¯çœŸå®çš„è®­ç»ƒæ•°æ®ï¼‰
        src = torch.randint(1, 1000, (batch_size, src_seq_len)).to(device)
        tgt_input = torch.randint(1, 1000, (batch_size, tgt_seq_len)).to(device)
        tgt_output = torch.randint(1, 1000, (batch_size, tgt_seq_len)).to(device)

        # åˆ›å»ºç›®æ ‡æ©ç 
        tgt_mask = model.generate_square_subsequent_mask(tgt_seq_len, device)
        tgt_mask = tgt_mask.unsqueeze(0).expand(batch_size, -1, -1)

        # å‰å‘ä¼ æ’­
        output = model(src, tgt_input, tgt_mask=tgt_mask)  # [batch_size, tgt_seq_len, vocab_size]

        # è®¡ç®—æŸå¤±
        loss = F.cross_entropy(
            output.reshape(-1, output.size(-1)),  # [batch_size * tgt_seq_len, vocab_size]
            tgt_output.reshape(-1),  # [batch_size * tgt_seq_len]
            ignore_index=0  # å¿½ç•¥å¡«å……token
        )

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # æ›´æ–°å‚æ•°
        optimizer.step()
        lr = scheduler.step()

        print(f"æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}: æŸå¤±={loss.item():.4f}, å­¦ä¹ ç‡={lr:.2e}")

        if device.type == 'cuda':
            memory_mb = torch.cuda.memory_allocated(device) / 1024 ** 2
            print(f"  GPUå†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")

    print("\nâœ… è®­ç»ƒç¤ºä¾‹å®Œæˆï¼")


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_transformer()
    test_individual_components()
    test_demonstrate_attention_patterns()
    test_performance_benchmark()
    test_training_example()

    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼Transformeræ¨¡å‹å®ç°éªŒè¯æˆåŠŸï¼")
    print("=" * 50)